{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be75bff9",
   "metadata": {},
   "source": [
    "## Dataset Information\n",
    "\n",
    "This dataset is from Hugging Face, it is the celebrity-1000 dataset which contains images of the top 1000 celebrities and can be used for Convolutional Neural Network tasks. It contains a total of 18,184 images at a 256x256 resolution. The link for the dataset is here: https://huggingface.co/datasets/tonyassi/celebrity-1000\n",
    "\n",
    "## Changelog\n",
    "### 23/03/2025\n",
    "- Figure out a way to view images with PIL and reading bytes.\n",
    "- Current method far too slow and inefficient, looking into vectorised approaches.\n",
    "- Image conversion is using up all system memory and freezing the entire OS... looking for alternative approaches.\n",
    "\n",
    "### 24/03/2025\n",
    "- Discovered that memory issues are stemming from BytesIO. This saves the images in an 'efficient' way in memory rather on disk to be quicker. However, since there is just so many images, even so-called efficient storage is not enough. This is what is using up all the memory. A new approach will be to process N images, save them to disk, clear memory and resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86c39292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing\n",
    "\n",
    "df = pq.read_table('data/data.parquet').to_pandas()\n",
    "\n",
    "# extremely slow and inefficient, do not use\n",
    "def decode_image(image):\n",
    "    image = Image.open(io.BytesIO(image['bytes']))\n",
    "    image = image.resize((256, 256))  # Resize to a consistent size\n",
    "    image = np.array(image)\n",
    "    image = image / 255.0  # Normalize the image to [0, 1] range\n",
    "    return image\n",
    "\n",
    "def decode_single_image(image_bytes):\n",
    "    \"\"\"Decode a single image from bytes.\"\"\"\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    image = image.resize((256, 256))\n",
    "    return np.array(image) / 255.0\n",
    "\n",
    "def decode_images_batch(df, batch_size=32, use_parallel=True, max_workers=None):\n",
    "    \"\"\"Process images in batches to avoid memory issues.\"\"\"\n",
    "    if max_workers is None:\n",
    "        max_workers = multiprocessing.cpu_count()\n",
    "    \n",
    "    total_images = len(df)\n",
    "    print(f\"Processing {total_images} images in batches of {batch_size}\")\n",
    "    \n",
    "    for i in range(0, total_images, batch_size):\n",
    "        batch_df = df.iloc[i:min(i+batch_size, total_images)]\n",
    "        batch_labels = batch_df['label'].to_numpy()\n",
    "        \n",
    "        if use_parallel:\n",
    "            image_bytes_list = [row['image']['bytes'] for _, row in batch_df.iterrows()]\n",
    "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                batch_images = list(executor.map(decode_single_image, image_bytes_list))\n",
    "        else:\n",
    "            batch_images = []\n",
    "            for _, row in batch_df.iterrows():\n",
    "                batch_images.append(decode_single_image(row['image']['bytes']))\n",
    "        \n",
    "        yield np.array(batch_images), batch_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7887c30e",
   "metadata": {},
   "source": [
    "The above method is far too memory inefficient. Trying alternative below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25cae58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_parquet_images(parquet_file, target_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Efficiently processes images from a Parquet file.\n",
    "\n",
    "    Args:\n",
    "        parquet_file: Path to the Parquet file.\n",
    "        target_size:  Tuple (width, height) for resizing.\n",
    "\n",
    "    Returns:\n",
    "        A list of processed images (as NumPy arrays).  Can be easily\n",
    "        modified to yield images one at a time, or to write directly\n",
    "        to a file/database.\n",
    "    \"\"\"\n",
    "\n",
    "    table = pq.read_table(parquet_file)\n",
    "    num_images = len(table)\n",
    "    processed_images = []\n",
    "\n",
    "    for i in range(num_images):\n",
    "        image_bytes = table['image'][i]['bytes'].as_py()  # Get bytes for the i-th image\n",
    "        image = Image.open(io.BytesIO(image_bytes)) # Use BytesIO to avoid file on disk\n",
    "        image = image.resize(target_size)            # Resize\n",
    "        image_array = np.array(image) / 255.0  # Convert to NumPy array and Normalize\n",
    "        processed_images.append(image_array)\n",
    "\n",
    "    return processed_images\n",
    "\n",
    "# file_path = \"data/data.parquet\"\n",
    "# images = process_parquet_images(file_path)\n",
    "                                    \n",
    "# print(images[0].shape)  # Check the shape of a processed image\n",
    "# print(images[0].min(),images[0].max()) #verify normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ada7a4",
   "metadata": {},
   "source": [
    "Trying new method of saving N images to disk after processing them. Using garbage collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d39bae34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 1/4\n",
      "Processed and saved batch 2/4\n",
      "Processed and saved batch 3/4\n",
      "Processed and saved batch 4/4\n",
      "Image processing complete.\n"
     ]
    }
   ],
   "source": [
    "import gc # garbage collection\n",
    "import os\n",
    "\n",
    "def process_and_save_images(parquet_file, output_dir, target_size=(256, 256), batch_size=100):\n",
    "    \"\"\"\n",
    "    Processes images from a Parquet file in batches, saving them to disk.\n",
    "\n",
    "    Args:\n",
    "        parquet_file: Path to the Parquet file.\n",
    "        output_dir: Directory to save processed images.\n",
    "        target_size: Tuple (width, height) for resizing.\n",
    "        batch_size: Number of images to process per batch.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    table = pq.read_table(parquet_file)\n",
    "    num_images = len(table)\n",
    "    num_batches = (num_images + batch_size - 1) // batch_size  # Calculate number of batches\n",
    "\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, num_images)\n",
    "\n",
    "        processed_images = []\n",
    "\n",
    "        for i in range(start_index, end_index):\n",
    "            try:\n",
    "                image_bytes = table['image'][i]['bytes'].as_py()\n",
    "                image = Image.open(io.BytesIO(image_bytes))\n",
    "                image = image.resize(target_size)\n",
    "                image_array = np.array(image) / 255.0  # Normalize\n",
    "                processed_images.append(image_array)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Save the processed images for this batch\n",
    "        for j, image_array in enumerate(processed_images):\n",
    "            image_index = start_index + j\n",
    "            filename = os.path.join(output_dir, f\"image_{image_index:05d}.npy\")  # e.g., image_00000.npy\n",
    "            np.save(filename, image_array)\n",
    "\n",
    "        # Explicitly clear memory\n",
    "        del processed_images\n",
    "        gc.collect() #force garbage collection\n",
    "\n",
    "        print(f\"Processed and saved batch {batch_num + 1}/{num_batches}\")\n",
    "\n",
    "    print(\"Image processing complete.\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "parquet_file = \"data/data.parquet\"  # Replace with your file path\n",
    "output_dir = \"data/images\"\n",
    "process_and_save_images(parquet_file, output_dir, batch_size=5000) # Batch size of 5000 fits inside memory comfortably"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
