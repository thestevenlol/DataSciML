{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db257b1d",
   "metadata": {},
   "source": [
    "## Dataset Information\n",
    "\n",
    "This dataset is from Hugging Face, it is the celebrity-1000 dataset which contains images of the top 1000 celebrities and can be used for Convolutional Neural Network tasks. It contains a total of 18,184 images at a 256x256 resolution. The link for the dataset is here: https://huggingface.co/datasets/tonyassi/celebrity-1000\n",
    "\n",
    "## Changelog\n",
    "### 23/03/2025\n",
    "- Figure out a way to view images with PIL and reading bytes.\n",
    "- Current method far too slow and inefficient, looking into vectorised approaches.\n",
    "- Image conversion is using up all system memory and freezing the entire OS... looking for alternative approaches.\n",
    "\n",
    "### 24/03/2025\n",
    "- Discovered that memory issues are stemming from BytesIO. This saves the images in an 'efficient' way in memory rather on disk to be quicker. However, since there is just so many images, even so-called efficient storage is not enough. This is what is using up all the memory. A new approach will be to process N images, save them to disk, clear memory and resume.\n",
    "- Kept on running into memory issues and can't seem to get around them. If the I had more time and a smaller dataset I would have been able to produce a more positive output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc5ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "df = pd.read_parquet('data/data.parquet')\n",
    "\n",
    "# extremely slow and inefficient, do not use\n",
    "def decode_image(image):\n",
    "    image = Image.open(io.BytesIO(image['bytes']))\n",
    "    image = image.resize((256, 256))  # Resize to a consistent size\n",
    "    image = np.array(image)\n",
    "    image = image / 255.0  # Normalize the image to [0, 1] range\n",
    "    return image\n",
    "\n",
    "def decode_single_image(image_bytes):\n",
    "    \"\"\"Decode a single image from bytes.\"\"\"\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    image = image.resize((256, 256))\n",
    "    return np.array(image) / 255.0\n",
    "\n",
    "def decode_images_batch(df, batch_size=32, use_parallel=True, max_workers=None):\n",
    "    \"\"\"Process images in batches to avoid memory issues.\"\"\"\n",
    "    if max_workers is None:\n",
    "        max_workers = multiprocessing.cpu_count()\n",
    "    \n",
    "    total_images = len(df)\n",
    "    print(f\"Processing {total_images} images in batches of {batch_size}\")\n",
    "    \n",
    "    for i in range(0, total_images, batch_size):\n",
    "        batch_df = df.iloc[i:min(i+batch_size, total_images)]\n",
    "        batch_labels = batch_df['label'].to_numpy()\n",
    "        \n",
    "        if use_parallel:\n",
    "            image_bytes_list = [row['image']['bytes'] for _, row in batch_df.iterrows()]\n",
    "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                batch_images = list(executor.map(decode_single_image, image_bytes_list))\n",
    "        else:\n",
    "            batch_images = []\n",
    "            for _, row in batch_df.iterrows():\n",
    "                batch_images.append(decode_single_image(row['image']['bytes']))\n",
    "        \n",
    "        yield np.array(batch_images), batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf090f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import io\n",
    "\n",
    "def process_parquet_images(parquet_file, target_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Efficiently processes images from a Parquet file.\n",
    "\n",
    "    Args:\n",
    "        parquet_file: Path to the Parquet file.\n",
    "        target_size:  Tuple (width, height) for resizing.\n",
    "\n",
    "    Returns:\n",
    "        A list of processed images (as NumPy arrays).  Can be easily\n",
    "        modified to yield images one at a time, or to write directly\n",
    "        to a file/database.\n",
    "    \"\"\"\n",
    "\n",
    "    table = pq.read_table(parquet_file)\n",
    "    num_images = len(table)\n",
    "    processed_images = []\n",
    "\n",
    "    for i in range(num_images):\n",
    "        image_bytes = table['image'][i]['bytes'].as_py()  # Get bytes for the i-th image\n",
    "        image = Image.open(io.BytesIO(image_bytes)) # Use BytesIO to avoid file on disk\n",
    "        image = image.resize(target_size)            # Resize\n",
    "        image_array = np.array(image) / 255.0  # Convert to NumPy array and Normalize\n",
    "        processed_images.append(image_array)\n",
    "\n",
    "    return processed_images\n",
    "\n",
    "# file_path = \"data/data.parquet\"\n",
    "# images = process_parquet_images(file_path)\n",
    "                                    \n",
    "# print(images[0].shape)  # Check the shape of a processed image\n",
    "# print(images[0].min(),images[0].max()) #verify normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d941b256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved batch 1/4\n",
      "Processed and saved batch 2/4\n",
      "Processed and saved batch 3/4\n",
      "Processed and saved batch 4/4\n",
      "Image processing complete.\n"
     ]
    }
   ],
   "source": [
    "import gc # garbage collection\n",
    "import os\n",
    "\n",
    "def process_and_save_images(parquet_file, output_dir, target_size=(256, 256), batch_size=100):\n",
    "    \"\"\"\n",
    "    Processes images from a Parquet file in batches, saving them to disk.\n",
    "\n",
    "    Args:\n",
    "        parquet_file: Path to the Parquet file.\n",
    "        output_dir: Directory to save processed images.\n",
    "        target_size: Tuple (width, height) for resizing.\n",
    "        batch_size: Number of images to process per batch.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    table = pq.read_table(parquet_file)\n",
    "    num_images = len(table)\n",
    "    num_batches = (num_images + batch_size - 1) // batch_size  # Calculate number of batches\n",
    "\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, num_images)\n",
    "\n",
    "        processed_images = []\n",
    "\n",
    "        for i in range(start_index, end_index):\n",
    "            try:\n",
    "                image_bytes = table['image'][i]['bytes'].as_py()\n",
    "                image = Image.open(io.BytesIO(image_bytes))\n",
    "                image = image.resize(target_size)\n",
    "                image_array = np.array(image) / 255.0  # Normalize\n",
    "                processed_images.append(image_array)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Save the processed images for this batch\n",
    "        for j, image_array in enumerate(processed_images):\n",
    "            image_index = start_index + j\n",
    "            filename = os.path.join(output_dir, f\"image_{image_index:05d}.npy\")  # e.g., image_00000.npy\n",
    "            np.save(filename, image_array)\n",
    "\n",
    "        # Explicitly clear memory\n",
    "        del processed_images\n",
    "        gc.collect() #force garbage collection\n",
    "\n",
    "        print(f\"Processed and saved batch {batch_num + 1}/{num_batches}\")\n",
    "\n",
    "    print(\"Image processing complete.\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "parquet_file = \"data/data.parquet\"  # Replace with your file path\n",
    "output_dir = \"data/images\"\n",
    "process_and_save_images(parquet_file, output_dir, batch_size=5000) # Batch size of 5000 fits inside memory comfortably"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f7598ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">254</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">254</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">230400</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,745,664</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">997</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">64,805</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m254\u001b[0m, \u001b[38;5;34m254\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m230400\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │    \u001b[38;5;34m14,745,664\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m997\u001b[0m)            │        \u001b[38;5;34m64,805\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,866,789</span> (56.71 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,866,789\u001b[0m (56.71 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,866,789</span> (56.71 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,866,789\u001b[0m (56.71 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "df = pq.read_table('data/data.parquet').to_pandas()\n",
    "\n",
    "# Get the unique labels from the 'label' column\n",
    "unique_labels = df['label'].unique()\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)), # Assuming RGB images\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax') # num_classes is the number of classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', # or categorical_crossentropy\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "810e5715",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "# Load images and labels from the saved .npy files\n",
    "for i in range(len(df)):\n",
    "    image_path = os.path.join(output_dir, f\"image_{i:05d}.npy\")\n",
    "    image_array = np.load(image_path)\n",
    "    train_images.append(image_array)\n",
    "    train_labels.append(df['label'][i])\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Normalize the images\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "# Split the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_images, train_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=10, batch_size=32,\n",
    "                    validation_data=(val_images, val_labels))\n",
    "\n",
    "# Save the model\n",
    "model.save('image_classification_model.h5')\n",
    "\n",
    "# Load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('image_classification_model.h5')\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(val_images, val_labels)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
